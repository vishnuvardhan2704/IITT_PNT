{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e23b66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:194: SyntaxWarning: invalid escape sequence '\\I'\n",
      "<>:195: SyntaxWarning: invalid escape sequence '\\I'\n",
      "<>:196: SyntaxWarning: invalid escape sequence '\\I'\n",
      "<>:197: SyntaxWarning: invalid escape sequence '\\I'\n",
      "<>:194: SyntaxWarning: invalid escape sequence '\\I'\n",
      "<>:195: SyntaxWarning: invalid escape sequence '\\I'\n",
      "<>:196: SyntaxWarning: invalid escape sequence '\\I'\n",
      "<>:197: SyntaxWarning: invalid escape sequence '\\I'\n",
      "C:\\Users\\vvard\\AppData\\Local\\Temp\\ipykernel_12432\\74403314.py:194: SyntaxWarning: invalid escape sequence '\\I'\n",
      "  gfz_file = Path('D:\\IIT T dataset\\sample-ghz.txt')\n",
      "C:\\Users\\vvard\\AppData\\Local\\Temp\\ipykernel_12432\\74403314.py:195: SyntaxWarning: invalid escape sequence '\\I'\n",
      "  tec_file = Path('D:\\IIT T dataset\\SCN_TEC_1min_2018_m03.dat')\n",
      "C:\\Users\\vvard\\AppData\\Local\\Temp\\ipykernel_12432\\74403314.py:196: SyntaxWarning: invalid escape sequence '\\I'\n",
      "  psn_file = Path('D:\\IIT T dataset\\eqTime_PSN_data_2018_m03.dat')\n",
      "C:\\Users\\vvard\\AppData\\Local\\Temp\\ipykernel_12432\\74403314.py:197: SyntaxWarning: invalid escape sequence '\\I'\n",
      "  si_file  = Path('D:\\IIT T dataset\\eqTime_SI_SCN_data_2018_m03.dat')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "IONOSPHERIC TEC PREDICTION DATA PROCESSING\n",
      "============================================================\n",
      "\n",
      "1. Loading GFZ solar/geomagnetic data...\n",
      "âœ… GFZ data loaded: 248 records\n",
      "\n",
      "2. Loading and processing SCINDA data...\n",
      "Reading individual files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vvard\\AppData\\Local\\Temp\\ipykernel_12432\\74403314.py:40: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  tec = pd.read_csv(tec_fp, sep=r'\\t')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEC shape: (44640, 3)\n",
      "TEC columns: ['time.stamp', 'TECR', 'TECR.calibrated']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vvard\\AppData\\Local\\Temp\\ipykernel_12432\\74403314.py:45: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  psn = pd.read_csv(psn_fp, sep=r'\\t')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PSN shape: (44640, 14)\n",
      "PSN columns: ['time.stamp', 'year', 'month', 'day', 'sec.f.midnight', 'h', 'm', 's', 'mean.x', 'mean.y', 'mean.z', 'sigma.x', 'sigma.y', 'sigma.z']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vvard\\AppData\\Local\\Temp\\ipykernel_12432\\74403314.py:50: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  si = pd.read_csv(si_fp, sep=r'\\t')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SI shape before averaging: (256043, 6)\n",
      "SI columns: ['time.stamp', 'az', 'el', 'L1S4', 'ROTI', 'PRN']\n",
      "TEC datetime range: 2018-03-01 00:00:00 to 2018-03-31 23:59:00\n",
      "PSN datetime range: 2018-03-01 00:00:00 to 2018-03-31 23:59:00\n",
      "SI datetime range: 2018-03-01 00:00:00 to 2018-03-31 23:59:00\n",
      "Averaging SI data from 10 observations per minute to 1 per minute...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vvard\\AppData\\Local\\Temp\\ipykernel_12432\\74403314.py:72: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  si['datetime_minute'] = si['datetime'].dt.floor('T')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SI shape after averaging: (44640, 6)\n",
      "Detecting TEC column names...\n",
      "Found TEC raw column: TECR\n",
      "Found TEC calibrated column: TECR.calibrated\n",
      "Merging datasets...\n",
      "Merged dataset shape: (44640, 14)\n",
      "Applying quality filters...\n",
      "Filtered out 16111 rows with invalid data\n",
      "Final SCINDA dataset shape: (28529, 14)\n",
      "Final columns after processing: ['datetime', 'TEC', 'lat', 'lon', 'alt', 'az', 'el', 'S4', 'ROTI', 'PRN', 'date', 'time']\n",
      "âœ… SCINDA data processed: 28529 records\n",
      "\n",
      "3. Merging with GFZ data...\n",
      "âœ… Final merged dataset: 28529 records\n",
      "Available columns: ['TEC', 'lat', 'lon', 'alt', 'az', 'el', 'S4', 'ROTI', 'PRN', 'date', 'time', 'Kp', 'ap', 'Ap', 'SN', 'F10_7_obs', 'F10_7_adj']\n",
      "Keeping columns: ['date', 'time', 'TEC', 'lat', 'lon', 'alt', 'az', 'el', 'S4', 'ROTI', 'PRN', 'Kp', 'ap', 'Ap', 'SN', 'F10_7_obs']\n",
      "\n",
      "âœ… Dataset saved to: final_merged_dataset.csv\n",
      " Final dataset shape: (28529, 16)\n",
      " Columns: ['date', 'time', 'TEC', 'lat', 'lon', 'alt', 'az', 'el', 'S4', 'ROTI', 'PRN', 'Kp', 'ap', 'Ap', 'SN', 'F10_7_obs']\n",
      "\n",
      "ðŸ“ˆ Sample Statistics:\n",
      "   â€¢ TEC range: -6.91 - 46.80 TECU\n",
      "   â€¢ Latitude range: 30.23Â° - 38.78Â°\n",
      "   â€¢ Kp range: 0.0 - 4.3\n",
      "   â€¢ F10.7 range: 66.6 - 69.3 s.f.u.\n",
      "\n",
      " Dataset ready for ML training!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def load_gfz(filepath: str) -> pd.DataFrame:\n",
    "    \"\"\"Read raw GFZ file (no headers) and explode it to 3-hour resolution.\"\"\"\n",
    "    cols = (\n",
    "        ['year', 'month', 'day', 'julian', 'dec_jul', 'carr_rot', 'bartels'] +\n",
    "        [f'Kp_{i}' for i in range(8)] +\n",
    "        [f'ap_{i}' for i in range(8)] +\n",
    "        ['Ap', 'SN', 'F10_7_obs', 'F10_7_adj', 'qual']\n",
    "    )\n",
    "    df = pd.read_csv(filepath, sep=r'\\s+', names=cols, engine='python')\n",
    "\n",
    "    # stack the 8 three-hour columns into long form\n",
    "    long_frames = []\n",
    "    for i in range(8):\n",
    "        tmp = df[['year', 'month', 'day',\n",
    "                  f'Kp_{i}', f'ap_{i}',\n",
    "                  'Ap', 'SN', 'F10_7_obs', 'F10_7_adj']].copy()\n",
    "        tmp['hour'] = i * 3\n",
    "        tmp.rename(columns={f'Kp_{i}': 'Kp', f'ap_{i}': 'ap'}, inplace=True)\n",
    "        long_frames.append(tmp)\n",
    "    gfz = pd.concat(long_frames, ignore_index=True)\n",
    "    gfz['datetime'] = (\n",
    "        pd.to_datetime(gfz[['year', 'month', 'day']]) +\n",
    "        pd.to_timedelta(gfz['hour'], unit='h')\n",
    "    )\n",
    "    return gfz[['datetime', 'Kp', 'ap', 'Ap', 'SN', 'F10_7_obs', 'F10_7_adj']]\n",
    "\n",
    "# â”€â”€ 2.  TEC / PSN / SI monthly data â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def load_scinda(tec_fp, psn_fp, si_fp) -> pd.DataFrame:\n",
    "    \"\"\"Merge TEC, PSN, SI on 1-minute timestamps and clean.\"\"\"\n",
    "    print(\"Reading individual files...\")\n",
    "    \n",
    "    # Read TEC data (1 observation per minute)\n",
    "    tec = pd.read_csv(tec_fp, sep=r'\\t')\n",
    "    print(f\"TEC shape: {tec.shape}\")\n",
    "    print(f\"TEC columns: {list(tec.columns)}\")\n",
    "    \n",
    "    # Read PSN data (1 observation per minute)\n",
    "    psn = pd.read_csv(psn_fp, sep=r'\\t')\n",
    "    print(f\"PSN shape: {psn.shape}\")\n",
    "    print(f\"PSN columns: {list(psn.columns)}\")\n",
    "    \n",
    "    # Read SI data (10 observations per minute)\n",
    "    si = pd.read_csv(si_fp, sep=r'\\t')\n",
    "    print(f\"SI shape before averaging: {si.shape}\")\n",
    "    print(f\"SI columns: {list(si.columns)}\")\n",
    "\n",
    "    # FIXED: Flexible timestamp parsing for all datasets\n",
    "    for d, name in [(tec, 'TEC'), (psn, 'PSN'), (si, 'SI')]:\n",
    "        try:\n",
    "            d['datetime'] = pd.to_datetime(d['time.stamp'], format='%d-%m-%Y %H:%M')\n",
    "        except ValueError:\n",
    "            try:\n",
    "                d['datetime'] = pd.to_datetime(d['time.stamp'], format='%Y-%m-%d %H:%M:%S')\n",
    "            except ValueError:\n",
    "                try:\n",
    "                    d['datetime'] = pd.to_datetime(d['time.stamp'], format='%Y-%m-%d %H:%M')\n",
    "                except ValueError:\n",
    "                    d['datetime'] = pd.to_datetime(d['time.stamp'], infer_datetime_format=True)\n",
    "        print(f\"{name} datetime range: {d['datetime'].min()} to {d['datetime'].max()}\")\n",
    "\n",
    "    # NEW: Average SI data to 1-minute resolution\n",
    "    print(\"Averaging SI data from 10 observations per minute to 1 per minute...\")\n",
    "    \n",
    "    # Round SI datetime to minute for grouping\n",
    "    si['datetime_minute'] = si['datetime'].dt.floor('T')\n",
    "    \n",
    "    # Average numerical columns, take mode for categorical (PRN)\n",
    "    si_avg = si.groupby('datetime_minute').agg({\n",
    "        'az': 'mean',\n",
    "        'el': 'mean',\n",
    "        'L1S4': 'mean',\n",
    "        'ROTI': 'mean',\n",
    "        'PRN': lambda x: x.mode()[0] if not x.mode().empty else x.iloc[0]\n",
    "    }).reset_index()\n",
    "    \n",
    "    si_avg.rename(columns={'datetime_minute': 'datetime'}, inplace=True)\n",
    "    print(f\"SI shape after averaging: {si_avg.shape}\")\n",
    "\n",
    "    # FIXED: Dynamic column detection for TEC data\n",
    "    print(\"Detecting TEC column names...\")\n",
    "    tec_columns = list(tec.columns)\n",
    "    \n",
    "    # Find TEC columns with flexible matching\n",
    "    tec_raw_col = None\n",
    "    tec_cal_col = None\n",
    "    \n",
    "    for col in tec_columns:\n",
    "        if 'TECR' in col and 'calibrated' not in col:\n",
    "            tec_raw_col = col\n",
    "        elif 'TECR' in col and 'calibrated' in col:\n",
    "            tec_cal_col = col\n",
    "        elif 'TEC' in col and 'calibrated' in col:\n",
    "            tec_cal_col = col\n",
    "    \n",
    "    print(f\"Found TEC raw column: {tec_raw_col}\")\n",
    "    print(f\"Found TEC calibrated column: {tec_cal_col}\")\n",
    "    \n",
    "    if not tec_cal_col:\n",
    "        raise ValueError(f\"Could not find calibrated TEC column in: {tec_columns}\")\n",
    "\n",
    "    # Merge datasets\n",
    "    print(\"Merging datasets...\")\n",
    "    \n",
    "    # Start with TEC data - use detected column names\n",
    "    tec_cols_to_use = ['datetime']\n",
    "    if tec_raw_col:\n",
    "        tec_cols_to_use.append(tec_raw_col)\n",
    "    if tec_cal_col:\n",
    "        tec_cols_to_use.append(tec_cal_col)\n",
    "    \n",
    "    merged = tec[tec_cols_to_use].copy()\n",
    "    \n",
    "    # Merge PSN data\n",
    "    psn_cols = ['datetime', 'mean.x', 'mean.y', 'mean.z', 'sigma.x', 'sigma.y', 'sigma.z']\n",
    "    merged = merged.merge(psn[psn_cols], on='datetime', how='inner')\n",
    "    \n",
    "    # Merge averaged SI data\n",
    "    si_cols = ['datetime', 'az', 'el', 'L1S4', 'ROTI', 'PRN']\n",
    "    merged = merged.merge(si_avg[si_cols], on='datetime', how='inner')\n",
    "    \n",
    "    print(f\"Merged dataset shape: {merged.shape}\")\n",
    "\n",
    "    # FIXED: Dynamic column renaming\n",
    "    rename_dict = {\n",
    "        'mean.x': 'lat', \n",
    "        'mean.y': 'lon', \n",
    "        'mean.z': 'alt',\n",
    "        'L1S4': 'S4'\n",
    "    }\n",
    "    \n",
    "    # Add TEC column renaming\n",
    "    if tec_cal_col:\n",
    "        rename_dict[tec_cal_col] = 'TEC'\n",
    "    \n",
    "    merged.rename(columns=rename_dict, inplace=True)\n",
    "\n",
    "    # Quality filtering\n",
    "    print(\"Applying quality filters...\")\n",
    "    initial_rows = len(merged)\n",
    "    \n",
    "    # Filter out invalid coordinates and low elevation\n",
    "    merged = merged[\n",
    "        (merged['sigma.x'].notna()) & (merged['sigma.x'] != 0) &\n",
    "        (merged['sigma.y'].notna()) & (merged['sigma.y'] != 0) &\n",
    "        (merged['sigma.z'].notna()) & (merged['sigma.z'] != 0) &\n",
    "        (merged['el'] >= 15)\n",
    "    ]\n",
    "    \n",
    "    print(f\"Filtered out {initial_rows - len(merged)} rows with invalid data\")\n",
    "    print(f\"Final SCINDA dataset shape: {merged.shape}\")\n",
    "\n",
    "    # Drop unnecessary columns\n",
    "    cols_to_drop = ['sigma.x', 'sigma.y', 'sigma.z']\n",
    "    if tec_raw_col and tec_raw_col in merged.columns:\n",
    "        cols_to_drop.append(tec_raw_col)\n",
    "    \n",
    "    # Only drop columns that exist\n",
    "    cols_to_drop = [col for col in cols_to_drop if col in merged.columns]\n",
    "    if cols_to_drop:\n",
    "        merged.drop(columns=cols_to_drop, inplace=True)\n",
    "\n",
    "    # Create date/time columns\n",
    "    merged['date'] = merged['datetime'].dt.strftime('%d-%m-%Y')\n",
    "    merged['time'] = merged['datetime'].dt.strftime('%H:%M')\n",
    "    \n",
    "    print(f\"Final columns after processing: {list(merged.columns)}\")\n",
    "    return merged\n",
    "\n",
    "# â”€â”€ 3.  Nearest-time merge (GFZ â†’ SCINDA) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def attach_gfz(scinda: pd.DataFrame, gfz: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"For each SCINDA row pick GFZ record with nearest datetime.\"\"\"\n",
    "    scinda = scinda.copy()\n",
    "    gfz_sorted = gfz.sort_values('datetime')\n",
    "    scinda['datetime'] = pd.to_datetime(scinda['date'] + ' ' + scinda['time'],\n",
    "                                        format='%d-%m-%Y %H:%M')\n",
    "    final = pd.merge_asof(\n",
    "        scinda.sort_values('datetime'),\n",
    "        gfz_sorted,\n",
    "        on='datetime',\n",
    "        direction='nearest'\n",
    "    )\n",
    "    return final.drop(columns='datetime')\n",
    "\n",
    "# â”€â”€ 4.  Main routine â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def main():\n",
    "    # â”€â”€ edit these four paths â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    gfz_file = Path('D:\\IIT T dataset\\sample-ghz.txt')\n",
    "    tec_file = Path('D:\\IIT T dataset\\SCN_TEC_1min_2018_m03.dat')\n",
    "    psn_file = Path('D:\\IIT T dataset\\eqTime_PSN_data_2018_m03.dat')\n",
    "    si_file  = Path('D:\\IIT T dataset\\eqTime_SI_SCN_data_2018_m03.dat')\n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "    print(\"=\" * 60)\n",
    "    print(\"IONOSPHERIC TEC PREDICTION DATA PROCESSING\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    print(\"\\n1. Loading GFZ solar/geomagnetic data...\")\n",
    "    gfz = load_gfz(gfz_file)\n",
    "    print(f\"âœ… GFZ data loaded: {len(gfz)} records\")\n",
    "    \n",
    "    print(\"\\n2. Loading and processing SCINDA data...\")\n",
    "    scinda = load_scinda(tec_file, psn_file, si_file)\n",
    "    print(f\"âœ… SCINDA data processed: {len(scinda)} records\")\n",
    "    \n",
    "    print(\"\\n3. Merging with GFZ data...\")\n",
    "    final_df = attach_gfz(scinda, gfz)\n",
    "    print(f\"âœ… Final merged dataset: {len(final_df)} records\")\n",
    "\n",
    "    # FIXED: Dynamic column selection\n",
    "    available_columns = list(final_df.columns)\n",
    "    desired_columns = [\n",
    "        'date', 'time', 'TEC',           # Target and temporal\n",
    "        'lat', 'lon', 'alt',             # Spatial\n",
    "        'az', 'el', 'S4', 'ROTI', 'PRN', # Satellite/scintillation\n",
    "        'Kp', 'ap', 'Ap', 'SN', 'F10_7_obs'  # Solar/geomagnetic\n",
    "    ]\n",
    "    \n",
    "    # Only keep columns that actually exist\n",
    "    final_columns = [col for col in desired_columns if col in available_columns]\n",
    "    print(f\"Available columns: {available_columns}\")\n",
    "    print(f\"Keeping columns: {final_columns}\")\n",
    "    \n",
    "    final_df = final_df[final_columns]\n",
    "\n",
    "    # Save to CSV\n",
    "    output_file = 'final_merged_dataset.csv'\n",
    "    final_df.to_csv(output_file, index=False)\n",
    "    \n",
    "    print(f\"\\nâœ… Dataset saved to: {output_file}\")\n",
    "    print(f\" Final dataset shape: {final_df.shape}\")\n",
    "    print(f\" Columns: {list(final_df.columns)}\")\n",
    "    \n",
    "    # Display sample statistics\n",
    "    if 'TEC' in final_df.columns:\n",
    "        print(\"\\nðŸ“ˆ Sample Statistics:\")\n",
    "        print(f\"   â€¢ TEC range: {final_df['TEC'].min():.2f} - {final_df['TEC'].max():.2f} TECU\")\n",
    "        print(f\"   â€¢ Latitude range: {final_df['lat'].min():.2f}Â° - {final_df['lat'].max():.2f}Â°\")\n",
    "        print(f\"   â€¢ Kp range: {final_df['Kp'].min():.1f} - {final_df['Kp'].max():.1f}\")\n",
    "        print(f\"   â€¢ F10.7 range: {final_df['F10_7_obs'].min():.1f} - {final_df['F10_7_obs'].max():.1f} s.f.u.\")\n",
    "    \n",
    "    print(\"\\n Dataset ready for ML training!\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59085c11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>TEC</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>alt</th>\n",
       "      <th>az</th>\n",
       "      <th>el</th>\n",
       "      <th>S4</th>\n",
       "      <th>ROTI</th>\n",
       "      <th>PRN</th>\n",
       "      <th>Kp</th>\n",
       "      <th>ap</th>\n",
       "      <th>Ap</th>\n",
       "      <th>SN</th>\n",
       "      <th>F10_7_obs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01-03-2018</td>\n",
       "      <td>00:00</td>\n",
       "      <td>9.15</td>\n",
       "      <td>38.779316</td>\n",
       "      <td>-9.139703</td>\n",
       "      <td>128.123200</td>\n",
       "      <td>205.837500</td>\n",
       "      <td>36.762500</td>\n",
       "      <td>0.072500</td>\n",
       "      <td>9.515000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.667</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "      <td>67.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01-03-2018</td>\n",
       "      <td>00:01</td>\n",
       "      <td>9.14</td>\n",
       "      <td>38.779323</td>\n",
       "      <td>-9.139702</td>\n",
       "      <td>126.733350</td>\n",
       "      <td>206.025000</td>\n",
       "      <td>36.812500</td>\n",
       "      <td>0.086250</td>\n",
       "      <td>9.847500</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.667</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "      <td>67.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01-03-2018</td>\n",
       "      <td>00:02</td>\n",
       "      <td>9.12</td>\n",
       "      <td>38.779321</td>\n",
       "      <td>-9.139703</td>\n",
       "      <td>127.373833</td>\n",
       "      <td>206.175000</td>\n",
       "      <td>36.837500</td>\n",
       "      <td>0.081250</td>\n",
       "      <td>9.778750</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.667</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "      <td>67.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01-03-2018</td>\n",
       "      <td>00:03</td>\n",
       "      <td>9.12</td>\n",
       "      <td>38.779319</td>\n",
       "      <td>-9.139709</td>\n",
       "      <td>127.070050</td>\n",
       "      <td>195.366667</td>\n",
       "      <td>33.888889</td>\n",
       "      <td>0.098889</td>\n",
       "      <td>9.242222</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.667</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "      <td>67.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01-03-2018</td>\n",
       "      <td>00:04</td>\n",
       "      <td>9.11</td>\n",
       "      <td>38.779324</td>\n",
       "      <td>-9.139710</td>\n",
       "      <td>127.703383</td>\n",
       "      <td>195.500000</td>\n",
       "      <td>33.944444</td>\n",
       "      <td>0.095556</td>\n",
       "      <td>10.596667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.667</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "      <td>67.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28524</th>\n",
       "      <td>31-03-2018</td>\n",
       "      <td>23:55</td>\n",
       "      <td>7.76</td>\n",
       "      <td>38.779312</td>\n",
       "      <td>-9.139703</td>\n",
       "      <td>128.412717</td>\n",
       "      <td>185.900000</td>\n",
       "      <td>50.040000</td>\n",
       "      <td>0.069000</td>\n",
       "      <td>9.615556</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.667</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "      <td>69.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28525</th>\n",
       "      <td>31-03-2018</td>\n",
       "      <td>23:56</td>\n",
       "      <td>7.76</td>\n",
       "      <td>38.779313</td>\n",
       "      <td>-9.139702</td>\n",
       "      <td>128.672133</td>\n",
       "      <td>186.220000</td>\n",
       "      <td>50.040000</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>9.813333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.667</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "      <td>69.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28526</th>\n",
       "      <td>31-03-2018</td>\n",
       "      <td>23:57</td>\n",
       "      <td>7.77</td>\n",
       "      <td>38.779312</td>\n",
       "      <td>-9.139702</td>\n",
       "      <td>128.079183</td>\n",
       "      <td>186.590000</td>\n",
       "      <td>50.070000</td>\n",
       "      <td>0.072000</td>\n",
       "      <td>9.947778</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.667</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "      <td>69.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28527</th>\n",
       "      <td>31-03-2018</td>\n",
       "      <td>23:58</td>\n",
       "      <td>7.29</td>\n",
       "      <td>38.779315</td>\n",
       "      <td>-9.139700</td>\n",
       "      <td>128.655283</td>\n",
       "      <td>186.960000</td>\n",
       "      <td>50.100000</td>\n",
       "      <td>0.063000</td>\n",
       "      <td>9.964444</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.667</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "      <td>69.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28528</th>\n",
       "      <td>31-03-2018</td>\n",
       "      <td>23:59</td>\n",
       "      <td>7.29</td>\n",
       "      <td>38.779316</td>\n",
       "      <td>-9.139698</td>\n",
       "      <td>129.184180</td>\n",
       "      <td>151.330000</td>\n",
       "      <td>50.100000</td>\n",
       "      <td>0.061000</td>\n",
       "      <td>10.306667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.667</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "      <td>69.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28529 rows Ã— 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             date   time   TEC        lat       lon         alt          az  \\\n",
       "0      01-03-2018  00:00  9.15  38.779316 -9.139703  128.123200  205.837500   \n",
       "1      01-03-2018  00:01  9.14  38.779323 -9.139702  126.733350  206.025000   \n",
       "2      01-03-2018  00:02  9.12  38.779321 -9.139703  127.373833  206.175000   \n",
       "3      01-03-2018  00:03  9.12  38.779319 -9.139709  127.070050  195.366667   \n",
       "4      01-03-2018  00:04  9.11  38.779324 -9.139710  127.703383  195.500000   \n",
       "...           ...    ...   ...        ...       ...         ...         ...   \n",
       "28524  31-03-2018  23:55  7.76  38.779312 -9.139703  128.412717  185.900000   \n",
       "28525  31-03-2018  23:56  7.76  38.779313 -9.139702  128.672133  186.220000   \n",
       "28526  31-03-2018  23:57  7.77  38.779312 -9.139702  128.079183  186.590000   \n",
       "28527  31-03-2018  23:58  7.29  38.779315 -9.139700  128.655283  186.960000   \n",
       "28528  31-03-2018  23:59  7.29  38.779316 -9.139698  129.184180  151.330000   \n",
       "\n",
       "              el        S4       ROTI  PRN     Kp  ap  Ap  SN  F10_7_obs  \n",
       "0      36.762500  0.072500   9.515000  1.0  0.667   3   6  12       67.2  \n",
       "1      36.812500  0.086250   9.847500  1.0  0.667   3   6  12       67.2  \n",
       "2      36.837500  0.081250   9.778750  1.0  0.667   3   6  12       67.2  \n",
       "3      33.888889  0.098889   9.242222  1.0  0.667   3   6  12       67.2  \n",
       "4      33.944444  0.095556  10.596667  1.0  0.667   3   6  12       67.2  \n",
       "...          ...       ...        ...  ...    ...  ..  ..  ..        ...  \n",
       "28524  50.040000  0.069000   9.615556  1.0  0.667   3   6  13       69.0  \n",
       "28525  50.040000  0.070000   9.813333  1.0  0.667   3   6  13       69.0  \n",
       "28526  50.070000  0.072000   9.947778  1.0  0.667   3   6  13       69.0  \n",
       "28527  50.100000  0.063000   9.964444  1.0  0.667   3   6  13       69.0  \n",
       "28528  50.100000  0.061000  10.306667  1.0  0.667   3   6  13       69.0  \n",
       "\n",
       "[28529 rows x 16 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "file_path = r'D:\\IIT T dataset\\final_merged_dataset.csv'  # Using raw string\n",
    "\n",
    "df = pd.read_csv(file_path)\n",
    "# pd.set_option('display.max_rows', None)\n",
    "# pd.set_option('display.max_columns', None)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d88d6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def70d58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce1eab6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
